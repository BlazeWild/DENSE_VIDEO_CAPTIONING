{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be6562d",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) Detailed Explanation\n",
    "\n",
    "This document explains how the `VisionTransformer` class works in PyTorch, focusing on the `__init__` and `forward` functions, with concrete examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f831c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int,\n",
    "    in_channels: int = 3):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        # Convolution to convert image patches into embeddings\n",
    "        self.conv1 = nn.Conv2d(\n",
    "        in_channels=in_channels,\n",
    "        out_channels=width,\n",
    "        kernel_size=patch_size,\n",
    "        stride=patch_size,\n",
    "        bias=False\n",
    "        )\n",
    "\n",
    "        # Scaling factor for initialization\n",
    "        scale = width ** -0.5\n",
    "\n",
    "        # Learnable class embedding (same for all images initially)\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "\n",
    "        # Learnable positional embedding (sequence length = num_patches + 1)\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "        scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width)\n",
    "        )\n",
    "\n",
    "        # LayerNorm before Transformer\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        # LayerNorm after Transformer\n",
    "        self.ln_post = LayerNorm(width)\n",
    "\n",
    "        # Projection matrix to final output dimension\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "    \n",
    "    \n",
    "    def forward(self, x: torch.Tensor, output_all_features: bool = False, output_attention_map: bool = False):\n",
    "        # Step 1: Convert image patches into embeddings using conv\n",
    "        x = self.conv1(x) # shape = [batch, width, grid, grid]\n",
    "        grid = x.size(2)\n",
    "\n",
    "\n",
    "        # Step 2: Flatten patches\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1) # shape = [batch, width, grid**2]\n",
    "        x = x.permute(0, 2, 1) # shape = [batch, num_patches, width]\n",
    "\n",
    "        # Step 3: Prepare class token\n",
    "        batch_class_token = self.class_embedding + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device)\n",
    "        x = torch.cat([batch_class_token, x], dim=1) # shape = [batch, num_patches+1, width]\n",
    "\n",
    "        # Step 4: Add positional embeddings\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "\n",
    "        # Step 5: LayerNorm before Transformer\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        # Step 6: Permute for Transformer input\n",
    "        x = x.permute(1, 0, 2) # shape: [seq_len, batch, width]\n",
    "\n",
    "        # Step 7: Pass through Transformer\n",
    "        x, attn = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2) # shape: [batch, seq_len, width]\n",
    "\n",
    "        # Step 8: Take class token embedding for output\n",
    "        cls_feature = self.ln_post(x[:, 0, :]) @ self.proj\n",
    "\n",
    "        # Step 9: Prepare outputs\n",
    "        outputs = (cls_feature,)\n",
    "        if output_all_features:\n",
    "            outputs += (x[:, 1:, :],) # patch embeddings\n",
    "        if output_attention_map:\n",
    "            outputs += (einops.rearrange(attn[:, :, :, 0, 1:], 'n_layers b n_heads (h w) -> n_layers b n_heads h w', h=grid, w=grid),)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4d0e3",
   "metadata": {},
   "source": [
    "### Explanation with Example\n",
    "\n",
    "* **Input image**: 2 images of size 6x6, 1 channel each\n",
    "* **Patch size**: 3x3 → each image has `(6/3)^2 = 4` patches\n",
    "* **Width (embedding dimension)**: 3\n",
    "* **Class embedding**: shape `(width,) = (3,)`, same for all images initially\n",
    "* **Positional embedding**: shape `(num_patches + 1, width) = (5, 3)`\n",
    "* **Convolution (`conv1`)**: converts each 3x3 patch to a vector of size `width` (3)\n",
    "* **Transformer**: processes sequence of patch embeddings plus class token\n",
    "* **Projection**: final output dimension of the class token after Transformer\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Step-by-step Explanation with Example\n",
    "\n",
    "#### **Input patches example**\n",
    "\n",
    "```python\n",
    "# Batch of 2 images, 4 patches per image, embedding dim = 3\n",
    "x = torch.tensor([\n",
    "    [[0.1,0.2,0.3], [0.4,0.5,0.6], [0.7,0.8,0.9], [1.0,1.1,1.2]],  # image 1\n",
    "    [[1.1,1.2,1.3], [1.4,1.5,1.6], [1.7,1.8,1.9], [2.0,2.1,2.2]]   # image 2\n",
    "])  # shape = (2, 4, 3)\n",
    "```\n",
    "\n",
    "### **Adding Class Token to Batch (Explanation Only)**\n",
    "\n",
    "#### **Input example**\n",
    "\n",
    "```python\n",
    "# Batch of 2 images, 4 patches per image, embedding dim = 3\n",
    "x = torch.tensor([\n",
    "    [[0.1,0.2,0.3], [0.4,0.5,0.6], [0.7,0.8,0.9], [1.0,1.1,1.2]],  # image 1\n",
    "    [[1.1,1.2,1.3], [1.4,1.5,1.6], [1.7,1.8,1.9], [2.0,2.1,2.2]]   # image 2\n",
    "])  # shape = (2, 4, 3)\n",
    "```\n",
    "\n",
    "#### **Class embedding**\n",
    "\n",
    "```python\n",
    "class_embedding = torch.tensor([0.5, 0.6, 0.7])  # shape = (width,) = (3,)\n",
    "```\n",
    "\n",
    "* Learnable vector, same for all images initially.\n",
    "\n",
    "#### **Expanding class token for batch**\n",
    "\n",
    "```python\n",
    "batch_class_token = class_embedding + torch.zeros(x.shape[0], 1, x.shape[-1])\n",
    "# shape = (2, 1, 3)\n",
    "```\n",
    "\n",
    "* `torch.zeros(batch_size, 1, width)` acts as a placeholder.\n",
    "* Adding `class_embedding` broadcasts it to each image in the batch.\n",
    "\n",
    "```\n",
    "batch_class_token = [\n",
    " [[0.5, 0.6, 0.7]],  # image 1\n",
    " [[0.5, 0.6, 0.7]]   # image 2\n",
    "]\n",
    "```\n",
    "\n",
    "#### **Concatenating class token with patches**\n",
    "\n",
    "```python\n",
    "x = torch.cat([batch_class_token, x], dim=1)  # shape = (2, 5, 3)\n",
    "```\n",
    "\n",
    "* Sequence per image: `[CLS, P1, P2, P3, P4]`\n",
    "* Now each image in the batch has its own copy of the class token prepended.\n",
    "\n",
    "✅ **Key points:**\n",
    "\n",
    "* Class embedding is **shared initially across all images**.\n",
    "* Each image gets a **separate copy** in the batch via broadcasting.\n",
    "* After Transformer, the class token becomes **image-specific** through attention to patches.\n",
    "\n",
    "\n",
    "* Sequence per image: `[CLS, P1, P2, P3, P4]`\n",
    "---\n",
    "\n",
    "#### **Add positional embeddings**\n",
    "\n",
    "* Positional embedding shape: `(5, 3)` → added to each batch\n",
    "\n",
    "#### **LayerNorm & Transformer**\n",
    "\n",
    "* Normalizes embeddings along width dimension\n",
    "* Transformer attends patches + class token\n",
    "\n",
    "#### **Class token output**\n",
    "\n",
    "* `cls_feature = ln_post(x[:, 0, :]) @ proj` → image-specific representation\n",
    "* Even though **initial class token was same for all images**, attention gives it unique info for each image\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Key Points\n",
    "\n",
    "1. **Class embedding**: same vector for all images, learnable, prepended as first token\n",
    "2. **Patch embeddings**: flattened and embedded via convolution\n",
    "3. **Sequence length**: `num_patches + 1` (including class token)\n",
    "4. **Positional embeddings**: added to sequence, learnable, shape `(num_patches+1, width)`\n",
    "5. **Transformer**: outputs updated embeddings for each token; class token summarizes image content\n",
    "6. **Final output**: class token after Transformer → projected to desired output dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50562528",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
